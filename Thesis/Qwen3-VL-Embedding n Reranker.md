# 1. 全体像

結論：高速に候補を集める“Embedding”と、精密に並べ替える“Reranker”の2段構え。
テキスト/画像/文書画像/動画を同一の表現空間で扱う。

```全体図
文書画像/テキスト/動画
          |
          | ① Embeddingでベクトル化（事前計算して保存できる）
          v
   [Vector Index / ANN検索]
          |
          | ② 類似Top-K候補を取得（高速）
          v
   (候補K件だけ)
          |
          | ③ Rerankerで精密スコアリング（遅いが精密）
          v
     [最終順位 / 最終割当]

```

# 2. Embeddingは何をしているのか

すなわち、Dual-Tower / Bi-encoderという、２本の軸を持つモデルで動いている。
Query（探したいもの） と Document（候補） を 別々にモデルへ入れて、
それぞれ 固定長ベクトルに変換し、2つのベクトルの近さ（主にcosine類似度）で「関連度っぽさ」を測る。

```Dual Tower
           ┌───────────────┐
Query ---->│ Encoder (Tower) │----> q_vec (ベクトル)
           └───────────────┘

           ┌───────────────┐
Doc  ----->│ Encoder (Tower) │----> d_vec (ベクトル)
           └───────────────┘

score = cosine(q_vec, d_vec)
```

## 2.1 実際に用いる埋め込みベクトルはどこから取る？
Embeddingは ベースモデル最終層の[EOS]トークンの隠れ状態を最終表現として取り出している。

各層 𝑙=1..𝐿

各位置 𝑡=1..𝑇

最終層の [EOS] トークンの隠れ状態とは、最終層 l＝L の出力（= last_hidden_state）
その中の [EOS]（文末を表す特殊トークン）が置かれた位置のベクトルのこと。下図の星の部分

```[EOS]の概念図

                  位置 t（トークンの並び）
        t=1     t=2     t=3         ...     t=T-1    t=T(EOS)
l=1   [ ● ]   [ ● ]   [ ● ]        ...     [ ● ]    [ ● ]
l=2   [ ● ]   [ ● ]   [ ● ]        ...     [ ● ]    [ ● ]
l=3   [ ● ]   [ ● ]   [ ● ]        ...     [ ● ]    [ ● ]
 ...    ...     ...     ...                 ...      ...
l=L   [ ● ]   [ ● ]   [ ● ]        ...     [ ● ]    [ ⭐︎ ]
```
## 2.2 なぜ最終層の [EOS] トークンの隠れ状態をとるだけで全文・全画像を代表できるのか

Qwen3-VL のバックボーンが **causal attention（自己回帰マスク）**を用いているから。
自己回帰モデルでは、位置tの表現はそれ以前の全トークンを参照して更新されるため、最後のトークンは、前にある全てを見ている。

## 2.3 「隠れ状態」って何ですか？
直感的には、**各トークンの“理解メモ**

例として
```
私は りんご を 食べた
```

をトークン列として入れると、モデルは各位置に対してこんな「内部メモ」を作ります：

「私は」位置の hidden state：主語っぽい・人間っぽい…などの特徴
「りんご」位置の hidden state：食べ物っぽい・目的語っぽい…などの特徴
「食べた」位置の hidden state：動詞・過去形・文末…などの特徴

実際には「食べ物っぽい」みたいなラベルが付いてるわけではなく、2048次元や4096次元の数値のベクトルとして表現されている。 


より具体的に、Qwen3-VL-EmbeddingはTransformerを用いているが、
Transformerは層が積み重なっていて、層ごとに「各トークンの表現」を更新する。

```
0層（埋め込み直後）：単語そのもの＋位置情報のベクトル
1層：周囲（前文脈）の情報を混ぜた表現
…
最終層：タスクに使いやすい、より抽象的な表現
```

この 各層の各位置で出るベクトルが hidden state 。

「隠れ状態」は昔のニューラルネット（RNNなど）由来の呼び方。
モデル内部の状態（外に直接見せる出力ではない）が、そこから最終出力（次トークン予測、分類、埋め込みなど）を作るという意味で “hidden” と呼ぶ。

# 3. Rerankerは何をしているのか
Single-Tower / Cross-encoderで動く。
Query と Doc を 一緒にモデルへ入れて、cross-attentionで「Queryの各要素」と「Docの各要素」を突き合わせる。
**“このペアは関連する？”**を精密に判定する。

```
[Instruction][Query][Doc] ---> Single Encoder (Cross-Attention) ---> score
```
Rerankerでは関連度スコアを 特別トークンyes/noの生成確率で表現している。

Transformerの注意機構（attention）は、ざっくりこう

 ・Q（Query）：どこを見たいか
・K（Key）：各トークンの「見出し」
・V（Value）：各トークンの「中身」

「Query側のトークンが、Doc側のトークンを見に行く」のが cross-attention のコア